{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bibliographic-kingdom",
   "metadata": {},
   "source": [
    "### Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "partial-jaguar",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.feature_selection import RFECV, RFE\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustained-impact",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "original-bubble",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv(\"data/new_data/old_records.csv.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connected-weekend",
   "metadata": {},
   "source": [
    "### Droping null values and discarded features from the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "convinced-castle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validity of the data:\n",
      "0 out of 66137 contains at least one null value representing 0.0 % of the instances\n"
     ]
    }
   ],
   "source": [
    "data_null_values = dataframe[dataframe.isna().any(axis=1)]\n",
    "len_null = len(data_null_values)\n",
    "len_data = len(dataframe)\n",
    "\n",
    "# Print message\n",
    "print(\"Validity of the data:\")\n",
    "print(f\"{len_null} out of {len_data} contains at least one null value\", end=\" \")\n",
    "print(f\"representing {len_null / len_data} % of the instances\")\n",
    "\n",
    "cleaned_data = dataframe.dropna(how='any', axis=0)\n",
    "float_columns = cleaned_data.select_dtypes(include=['float64'])\n",
    "\n",
    "for col in float_columns.columns.values:\n",
    "    cleaned_data[col] = cleaned_data[col].astype('int64')\n",
    "    \n",
    "previous_discarded_features = ['TaskFileRelocation', 'RoleVariableRelocation', 'HandlerBlockEdit', \n",
    "                               'DefaultVarFileRemoval', 'DefaultVarFileRelocation', 'HandlerFileRemoval', \n",
    "                               'DefaultVariableRelocation', 'HandlerBlockRelocation', 'RoleVarFileRelocation', \n",
    "                               'HandlerFileRelocation', 'HandlerTaskRemoval', 'HandlerFileAddition']\n",
    "\n",
    "excluded_data = ['id', 'v1', 'v2', 'HandlerBlockAddition'] + previous_discarded_features\n",
    "\n",
    "filtered_data = cleaned_data.drop(excluded_data, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "early-contrary",
   "metadata": {},
   "source": [
    "### Transform data into Numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "fifty-universe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shapes of the data:\n",
      "(66137, 28)\n",
      "(66137,)\n"
     ]
    }
   ],
   "source": [
    "to_transform = {\"release\": {\"patch\": 0, \"minor\": 1, \"major\": 2}}\n",
    "filtered_data.replace(to_transform, inplace=True)\n",
    "\n",
    "# Transforming data into Numpy arrays\n",
    "X = filtered_data[filtered_data.columns[:-1]].to_numpy()\n",
    "y = filtered_data[filtered_data.columns[-1]].to_numpy()\n",
    "\n",
    "# Print shapes\n",
    "print()\n",
    "print(\"Shapes of the data:\")\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conceptual-pantyhose",
   "metadata": {},
   "source": [
    "### Configuring Undersampling Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "nearby-thesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler, NearMiss, NeighbourhoodCleaningRule, TomekLinks, OneSidedSelection\n",
    "\n",
    "# undersampler = RandomUnderSampler()\n",
    "# undersampler = NearMiss()\n",
    "# undersampler = NeighbourhoodCleaningRule(n_jobs=-1)\n",
    "undersampler = TomekLinks(n_jobs=-1)\n",
    "# undersampler = OneSidedSelection(n_neighbors=5, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spread-valuation",
   "metadata": {},
   "source": [
    "### Configuring classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "minute-rebound",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_jobs=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "academic-maker",
   "metadata": {},
   "source": [
    "### Making pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "scientific-composite",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import make_pipeline\n",
    "\n",
    "pipeline = make_pipeline(undersampler, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "small-roads",
   "metadata": {},
   "source": [
    "### Stratified Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "promotional-inventory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean of Precisions:  0.7228779637924349\n",
      "Mean of Recalls:  0.565713583471212\n",
      "Confusion matrix:\n",
      "[[44856  2884   233]\n",
      " [ 8474  6315   216]\n",
      " [ 1499   582  1078]]\n"
     ]
    }
   ],
   "source": [
    "cv = StratifiedKFold(n_splits=10)\n",
    "\n",
    "precision_scores = list()\n",
    "recall_scores = list()\n",
    "confusion_matrices = list()\n",
    "\n",
    "for i, (train, test) in enumerate(cv.split(X, y)):\n",
    "    y_predict = pipeline.fit(X[train], y[train]).predict(X[test])\n",
    "\n",
    "    precision_value = precision_score(y[test], y_predict, average=\"macro\")\n",
    "    recall_value = recall_score(y[test], y_predict, average=\"macro\")\n",
    "    conf_matrix = confusion_matrix(y[test], y_predict)\n",
    "\n",
    "    precision_scores.append(precision_value)\n",
    "    recall_scores.append(recall_value)\n",
    "    confusion_matrices.append(conf_matrix)\n",
    "\n",
    "print()\n",
    "print(\"Mean of Precisions: \", np.mean(precision_scores))\n",
    "print(\"Mean of Recalls: \", np.mean(recall_scores))\n",
    "    \n",
    "print(\"Confusion matrix:\")\n",
    "print(sum(confusion_matrices))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floating-likelihood",
   "metadata": {},
   "source": [
    "### Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "qualified-tutorial",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "pickle.dump(clf, open(\"data/models/randomForest_old_data.pickle\", \"wb\"))\n",
    "\n",
    "# Data Transformed\n",
    "pickle.dump(X, open(\"data/transformed_data/X_train_old.pickle\", \"wb\"))\n",
    "pickle.dump(y, open(\"data/transformed_data/y_train_old.pickle\", \"wb\"))\n",
    "\n",
    "# Feature-related\n",
    "pickle.dump(column_names, open(\"data/features/features.pickle\", \"wb\"))\n",
    "pickle.dump(rfecv.ranking_, open(\"data/features/ranking_features.pickle\", \"wb\"))\n",
    "pickle.dump(selected_features, open(\"data/features/selected_features.pickle\", \"wb\"))\n",
    "pickle.dump(discarded_features, open(\"data/features/discarded_features.pickle\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
